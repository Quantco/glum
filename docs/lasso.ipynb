{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "f(\\beta_0, \\beta) &= \\frac{1}{2N} \\sum_{i=1}^N \\left(y_i - \\beta_0 - x_i^T \\beta \\right)^2 + \\lambda P_\\alpha(\\beta) \\\\\n",
    "\\frac{\\partial f}{\\partial \\beta_0} &= \n",
    "-\\frac{1}{N} \\sum_{i=1}^N \\left(y_i - \\beta_0 - x_i^T \\beta \\right) \\\\\n",
    "\\beta_0^*(\\beta) &= \\frac{1}{N} \\sum_{i=1}^N \\left(y_i - x_i^T \\beta \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\beta_j = 0$, the gradient is not defined. If $\\beta_j \\neq 0$, then\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial \\beta_j} &= \n",
    "\\frac{-1}{N} \\sum_i x_{ij} \\left(y_i - \\beta_0 - x_i^T \\beta \\right) \n",
    "+ \\lambda \\frac{\\partial P_\\alpha(\\beta)}{\\partial \\beta_j} \\\\\n",
    "&= \n",
    "\\frac{-1}{N} \\sum_i x_{ij} \\left(y_i - \\beta_0 - x_i^T \\beta \\right) \n",
    "+ \\lambda \\left( (1 - \\alpha) \\beta_j + \\alpha \\mathrm{sign}(\\beta_j)\n",
    "\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal $\\beta_j$ will be zero if the gradient to the right of 0 is negative and the gradient to right is positive. That is,\n",
    "\\begin{align*}\n",
    "- \\lambda \\alpha &> \\frac{1}{N} \\sum_i x_{ij} \\left(y_i - \\beta_0 - x_i^T \\beta \\right) > \\lambda \\alpha \\\\\n",
    "\\mathrm{abs} \\left( \\frac{1}{N} \\sum_i x_{ij} \\left(y_i - \\beta_0 - x_i^T \\beta \\right) \\right)  &< \\lambda \\alpha\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the optimal value of $\\beta_j$ is not zero, we can find it by setting the gradient equal to zero:\n",
    "\\begin{align*}\n",
    "0 &= \n",
    "\\frac{-1}{N} \\sum_i x_{ij} \\left(\n",
    "y_i - \\beta_0 - \\tilde{x}^T_{ij} \\tilde{\\beta}_j - x_{ij} \\beta_j \\right)\n",
    "+ \\lambda \\left( (1 - \\alpha) \\beta_j + \\alpha \\mathrm{sign}(\\beta_j) \\right) \\\\\n",
    "\\beta_j \\left( \\frac{1}{N} \\sum_i x_{ij}^2 + \\lambda(1 - \\alpha) \\right) &= \n",
    "\\frac{-1}{N} \\sum_i x_{ij} \\left(\n",
    "y_i - \\beta_0 - \\tilde{x}^T_{ij} \\tilde{\\beta}_j \\right)\n",
    "- \\lambda \\alpha \\mathrm{sign}(\\beta_j) \\\\\n",
    "\\beta_j^*  &= \n",
    "\\frac{\\frac{1}{N} \\sum_i x_{ij} \\left(\n",
    "y_i - \\beta_0 - \\tilde{x}^T_{ij} \\tilde{\\beta}_j \\right)\n",
    "- \\lambda \\alpha \\mathrm{sign}(\\beta_j) }\n",
    "{\\frac{1}{N} \\sum_i x_{ij}^2 \n",
    "+ \\lambda (1 - \\alpha) }\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Updates (Section 2.1)\n",
    "\n",
    "\\begin{align*}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the soft-thresholding operator\n",
    "\\begin{align*}\n",
    "\\beta_j^*  &= \\frac\n",
    "{S \\left( \\frac{1}{N} \\sum_i x_{ij} \\left(y_i - \\beta_0 - \\tilde{x}_{ij}^T \\tilde{\\beta}_j \\right), \\lambda \\alpha \\right)}\n",
    "{\\frac{1}{N} \\sum_i x_{ij}^2 + \\lambda (1 - \\alpha) }\n",
    "\\end{align*}\n",
    "In the \"naive\" optimizer, $x$ is normalized so that $\\frac{1}{N}\\sum_i x_{ij}^2 = 1$. In the sparse optimizer, it is not so clear what is going on, but I think $x$ is scaled so that the same property still holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "$z = \\frac{1}{N} \\sum_i x_{ij} (y_i - \\beta_0 - \\tilde{x}_{ij}^T \\tilde{\\beta}_j)$. Let's look at $z$ more closely, defining residuals $r$.\n",
    "\\begin{align*}\n",
    "z &= \\frac{1}{N}  \\sum_i x_{ij} (y_i - \\beta_0 - \\tilde{x}_{ij}^T \\tilde{\\beta}_j) \\\\\n",
    "&= \\frac{1}{N} \\sum_i x_{ij} (r_i + x_{ij} \\beta_j)\\\\\n",
    "&= \\frac{1}{N} \\sum_i x_{ij} r_i + \\frac{1}{N} \\beta_j \\sum_i x_{ij}^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have normalized so that $\\frac{1}{N} \\sum_i x_{ij}^2 = 1$, then we can simplify the above equations:\n",
    "\\begin{align*}\n",
    "z &= \\frac{1}{N} \\sum_i x_{ij} r_i + \\beta_j \\\\\n",
    "\\beta_j^* &= \\frac{S(z, \\lambda \\alpha)}{1 + \\lambda (1 - \\alpha)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cw_update(y, x, j, beta, alpha, lambda_):\n",
    "    prediction_not_j = x.dot(beta) - x[:, j] * beta[j]\n",
    "    resid = y - prediction_not_j\n",
    "    n = len(y)\n",
    "    mean_resid = x[:, j].dot(resid) / n\n",
    "    numerator = soft_threshold(mean_resid, lambda_ * alpha)\n",
    "    denominator = 1 + lambda_ * (1 - alpha)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd_update(y, x, beta, alpha, lamba_):\n",
    "    for i in range(len(beta)):\n",
    "        beta[i] = get_cw_update(y, x, j, beta, alpha, lambda_)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cd(y, x, alpha, lambda_, n_iters):\n",
    "    beta = np.zeros(x.shape[1])\n",
    "    for i in range(n_iters):\n",
    "        beta = cd_update(y, x, beta, alpha, lambda_)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "LL &= \\sum_i LL_i(\\theta(x_i^T \\beta))\\\\\n",
    "\\frac{\\partial LL_i}{\\partial \\beta} &= LL_i'(\\theta(x_i^T\\beta)) \\theta'(x_i^T\\beta) x_i \\\\\n",
    "&\\equiv LL_i' \\theta' x_i \\\\\n",
    "\\frac{\\partial^2 LL_i}{\\partial \\beta \\partial \\beta^T} &= \\left(\n",
    "LL_i'' \\theta'^2 + LL_i \\theta '' \\right) x_i x_i^T\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taylor expand:\n",
    "\n",
    "\\begin{align*}\n",
    "LL_i(\\beta) &\\approx LL_i(\\tilde{\\beta}) + LL_i' \\theta' x_i^T \\left( \\beta  - \\tilde{\\beta} \\right)\n",
    "+ \\frac{1}{2} \\left(\n",
    "LL_i'' \\theta'^2 + LL_i \\theta '' \\right) \\left(x_i^T \\beta - x_i^T \\tilde{\\beta} \\right)^2 \\\\\n",
    "&= C(\\tilde{\\beta}) + LL_i' \\theta' x_i^T \\beta\n",
    "+ \\frac{1}{2} \\left(\n",
    "LL_i'' \\theta'^2 + LL_i \\theta '' \\right) \\left((x_i^T \\beta)^2 - 2 x_i^T \\tilde{\\beta} x_i^T \\beta \\right) \\\\\n",
    "&= C(\\tilde{\\beta}) + \\frac{1}{2} \\left(\n",
    "LL_i'' \\theta'^2 + LL_i \\theta '' \\right) \\left((x_i^T \\beta)^2 - 2 x_i^T \\tilde{\\beta} x_i^T \\beta \n",
    "+ 2 \\frac{LL_i' \\theta'}{LL_i'' \\theta'^2 + LL_i \\theta''} x_i^T \\beta\n",
    "\\right) \\\\\n",
    "&= C(\\tilde{\\beta}) + \\frac{1}{2} \\left(\n",
    "LL_i'' \\theta'^2 + LL_i \\theta '' \\right) \\left(x_i^T \\beta - \\left( x_i^T \\tilde{\\beta} \n",
    "- \\frac{LL_i' \\theta'}{LL_i'' \\theta'^2 + LL_i \\theta''} \\right)\n",
    "\\right)^2 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "w_i &= \\frac{1}{2} \\left(\n",
    "LL_i'' \\theta'^2 + LL_i \\theta '' \\right) \\\\\n",
    "z_i &=  x_i^T \\tilde{\\beta} \n",
    "- \\frac{LL_i' \\theta'}{LL_i'' \\theta'^2 + LL_i \\theta''}  \\\\\n",
    "&= x_i^T \\tilde{\\beta}\n",
    "- \\frac{LL_i' \\theta'}{2 w_i}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful derivatives:\n",
    "\n",
    "Gaussian:\n",
    "\n",
    "\\begin{align*}\n",
    "LL_i &= \\frac{1}{2} (y_i - \\theta_i)^2 \\\\\n",
    "LL_i' &= \\theta_i - y_i \\\\\n",
    "LL_i'' &= 1\n",
    "\\end{align*}\n",
    "\n",
    "Identity link:\n",
    "\\begin{align*}\n",
    "\\theta &= \\eta \\\\\n",
    "\\theta' &= 1 \\\\\n",
    "\\theta'' &= 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IRLS for Gaussian with identity link:\n",
    "\\begin{align*}\n",
    "w_i &= \\frac{1}{2} \\\\\n",
    "z_i &= x^T \\tilde{\\beta} - x^T \\tilde{\\beta} - y_i \\\\\n",
    "&= y_i \\\\\n",
    "\\min &\\sum_i \\frac{1}{2} \\left(y_i - x_i^T \\beta \\right)^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
