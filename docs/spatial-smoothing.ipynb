{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Geographic data in GLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting use case for GLMs with regularization is to use geographic data in forecasting tasks. Say you want to predict risk for car insurance. Accounting for geography is important, because where we people live impacts their frequency and severity of accidents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:\n",
    " - higher frequency of accidents in urban areas, lower severity on average\n",
    " - more weather related accidents near the coasts or mountains\n",
    " - higher severity of accidents in wealthier areas, because repair costs are higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While one could try and find more direct predictors for the examples above it is very convenient to simply use zip code as a predictor. The problem with using zip code is that this is a very high dimensional fixed effect and some zip codes may have very little exposure (i.e. very few observations). So estimates for some zip codes may be very noisy (or not defined) and fairly precise for others. Another problem with zip codes is that they tell us where drivers live, not necessarily where the drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The traditional approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, actuaries use spatial smoothing to reign in the zip code fixed effects. The (simplified) process works as follows:\n",
    "- Train a GLM such as $E[y|x] = \\exp(X' \\beta)$\n",
    "- Retrieve zip code level residuals from this model (as in the zip code fixed effects that are obtained using $X'\\hat \\beta$ as offset)\n",
    "- Spatially smooth the zip code level residuals using some kernel, e.g. Gaussian. We take into account the exposure of zip codes and the distance between zip codes.\n",
    "- Take the smooth zip code level residuals and cluster them into 30 zones using, e.g., Ward clustering.\n",
    "- Take the zones, one-hot encode them, call them $Z$, and fit the GLM with them: $E[y|x] = \\exp(X' \\beta + Z' \\gamma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the smoothing work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $r_j$ refers to the residual of region $i$\n",
    "- $d_{jj'}$ refers to the distance between regions $j$ and $j'$\n",
    "- $e_j$ refers to exposure of region $j$\n",
    "- $f(\\cdot)$ is a Kernel that we use for weighting\n",
    " \n",
    "The smoothed residual is given by:\n",
    " \n",
    "\\begin{align}\n",
    "\\tilde {r_j} = w_j r_j + (1 - w_j) \\frac{\\sum_{j \\neq i} f(d_{jj'}) e_j r_j}{\\sum_{j \\neq i} f(d_{jj'}) e_j}\n",
    "\\end{align}\n",
    " \n",
    "where $w_j$ refers to the weight placed on one's own average:\n",
    " \n",
    "\\begin{align}\n",
    "w_j = \\left(\\frac{e_j}{\\sum_{j \\neq i} f(d_{jj'}) e_j}\\right) ^ \\rho\n",
    "\\end{align}\n",
    " \n",
    "$f(\\cdot)$ can be any kind of Kernel; here we use a Gaussian Kernel:\n",
    " \n",
    "\\begin{align}\n",
    "f(d) = \\exp\\left( -\\frac{1}{2} \\left(\\frac{d}{h}\\right)^2 \\right),\n",
    "\\end{align}\n",
    "where $h$ refers to the bandwidth.\n",
    " \n",
    "In this form, the smoother has two hyperparameters $h$ and $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do we get the residuals from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the correct residual for smoothing?\n",
    " \n",
    "Index observations by $i$ and regions by $j$. All individuals in region $j$\n",
    "are given by $I(j)$. The region of observation $i$ is given by $j(i)$.\n",
    " \n",
    "Using a log link function, the conditional expectation of $y_i/e_i$ is modeled as\n",
    "$$\n",
    "\\mathbb E[y_i / e_i | x_i] = \\exp(x_i' \\beta),\n",
    "$$\n",
    "where $e_i$ denotes exposure.\n",
    " \n",
    "We run this model, which gives us $\\hat{\\beta}$.\n",
    " \n",
    "So what's the correct residual to use? Note that we want the residual on the\n",
    "zip code level. So we _define_ the residual as the $\\alpha_j$ that solve\n",
    "$$\n",
    "\\mathbb E[y_i / e_i | x_i] = \\exp(x_i' \\hat{\\beta} + \\alpha_j).\n",
    "$$\n",
    " \n",
    "How to estimate $\\{\\alpha_j\\}_j$?\n",
    " \n",
    "Using $e_i$ to denote exposure, the interesting bit of the log-likelihood of a\n",
    "Tweedie GLM with power $p \\in (1, 2)$ is given by\n",
    "$$\\sum_i e_i \\left(\\frac{y_i}{e_i} \\frac{\\exp(x_i' \\hat{\\beta} + \\alpha_j) ^ {1-p}}{1-p} - \\frac{\\exp(x_i' \\hat{\\beta} + \\alpha_j) ^ {2 - p}}{2-p}\\right),$$\n",
    "where we use $e_i$ as weights. We skip the expression for\n",
    "$p=1$ and $p=2$, but the first order conditions are the same.\n",
    " \n",
    "The first order condition with respect to $\\alpha_j$ is given by, for all j:\n",
    " \n",
    "$$\\sum_{i \\in I(j)} e_i \\left( \\frac{y_i}{e_i} \\exp(x_i' \\hat{\\beta} + \\alpha_j)^{1-p} - \\exp(x_i' \\hat{\\beta} + \\alpha_j)^{2-p} \\right) = 0.$$\n",
    " \n",
    "Re-arranging, we get\n",
    " \n",
    "$$\\sum_{i \\in I(j)} y_i \\exp(x_i' \\hat{\\beta} + \\alpha_j)^{1-p} = \\sum_{i \\in I(j)} e_i \\exp(x_i' \\hat{\\beta} + \\alpha_j)^{2-p}$$\n",
    " \n",
    "and then\n",
    " \n",
    "$$\\exp(\\alpha_j)^{1-p} \\sum_{i \\in I(j)} y_i \\exp(x_i' \\hat{\\beta})^{1-p} = \\exp(\\alpha_j)^{2-p} \\sum_{i \\in I(j)} e_i \\exp(x_i' \\hat{\\beta})^{2-p},$$\n",
    " \n",
    "which means we can solve for $\\alpha_j$ directly:\n",
    " \n",
    "$$\\alpha_j = \\log \\left( \\sum_{i \\in I(j)} y_i \\exp(x_i' \\hat{\\beta})^{1-p} \\right) - \\log \\left( \\sum_{i \\in I(j)} e_i \\exp(x_i' \\hat{\\beta})^{2-p} \\right).$$\n",
    " \n",
    "For Poisson ($p=1$) this simplfies to:\n",
    " \n",
    "$$\\alpha_j = \\log \\left( \\sum_{i \\in I(j)} y_i \\right) - \\log \\left( \\sum_{i \\in I(j)} e_i \\exp(x_i' \\hat{\\beta}) \\right).$$\n",
    " \n",
    "For Gamma ($p=2$) this simplfies to:\n",
    " \n",
    "$$\\alpha_j = \\log \\left( \\sum_{i \\in I(j)} y_i \\exp(-x_i' \\hat{\\beta}) \\right) - \\log \\left( \\sum_{i \\in I(j)} e_i \\right).$$\n",
    " \n",
    "Note that $\\alpha_j$ is a zip-code level fixed effect. However, unlike a\n",
    "typical fixed effect, it is estimated sequentially, i.e. we're not jointly\n",
    "estimating the main effects $\\beta$. To the extent that regionally effects\n",
    "can be explained by $x_i$, the model will load on $\\beta$ not $\\alpha_j$.\n",
    " \n",
    "Note that for Poisson, we have a bit of an issue for all zip codes $j$ for\n",
    "which $y_i = 0$ for all $i \\in I(j)$, because the fixed effects would be\n",
    "$-\\infty$. As a first pass, we may just want to hard code a lower bound.\n",
    "It would be cleaner to use a Bayesian approach (or something similar to a\n",
    "Bayesian approach, such as a credibility method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Typical extensions of this approach involve using some geographic features (e.g., does the person live in a city? average income of the zip code?) in the first stage model and only smoothing the residual that is not easily explained by such variables. This also has the advantage of not treating areas that are very close but very different as (almost) equal.\n",
    "- Other extensions involve different distance metrics:\n",
    "    - as the crow flies\n",
    "    - actual driving times from centroid to centroid\n",
    "    - neighborhood based distance measure: how many borders to I have to cross to travel from $j$ to $j'$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using Regularized GLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the multi-step procedure outlined above, we can also do all of this in one go. See Section 6 in [this tutorial](https://github.com/lorentzenchr/Tutorial_freMTPL2/blob/b99b688f4be3c50d9a3356cc95bc4504742040d0/glm_freMTPL2_example.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb E[y_{ij} / e_{ij} | x_{ij}] = \\exp(x_{ij}' \\beta + \\alpha_j)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use $i$ to index observations and $j$ to index regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we estimate the GLM, we use L2 regularization on the vector of zip code effects $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\min_{\\alpha, \\beta} -\\mathcal L + \\lambda \\alpha P \\alpha'\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathcal L$ is the log-likelihood and $\\alpha$ is a $1 \\times J$ vector and $P$ is $J \\times J$ matrix. $\\lambda$ is a scalar (for convenience, can also be absorbed by $P$). This is known as [Tikhonov Regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating $P$ is straightforward but a little but of work. We want the differences between $\\alpha_j$ and $\\alpha_j'$ to be regularized. $\\alpha_j$ and $\\alpha_j'$ may be further apart from each other when regions $j$ and $j'$ are far apart from each other or when $j$ and $j'$ have lots of exposure. The weight matrix $P$ can encode the (exposure-weighted) distance between zip code areas (same as above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes this tricky?\n",
    "- We have a very many zip codes ($\\approx10,000$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps\n",
    "- Start with [this tutorial](https://github.com/lorentzenchr/Tutorial_freMTPL2/blob/b99b688f4be3c50d9a3356cc95bc4504742040d0/glm_freMTPL2_example.ipynb), familiarize yourself with the problem and adopt it to work with our code base\n",
    "- Extend the neighborhood based distance metric used there to a distance based metric\n",
    "- Compare the results with the traditional step-by-step wise approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Find (or generate) a data set for a prediction task that has high dimensional regional fixed effect\n",
    "- Figure out how to handle the large number of fixed effects (e.g. do not construct the full hessian)\n",
    "- Construct the weight matrix $P$ (consider preserving sparsity for regions that are far apart)\n",
    "- Figure out how to effectively tune the various hyperparameters that we have ($\\alpha$ and the parameters that went into the construction of $P$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "- There's no need to restrict this to L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
