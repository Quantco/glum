{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using geographic data in GLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting use case for GLMs with regularization is to incorporate geographic data in forecasting tasks. Say you want to predict risk for car insurance. It is important to take geography into account, because where people live impacts the frequency with which they have accidents as well as the severity of those accidents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:\n",
    " - higher frequency of accidents in urban areas, lower severity on average;\n",
    " - more weather-related accidents near the coasts or in the mountains;\n",
    " - higher severity of accidents in wealthier areas, because repair costs are higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While one could try and find more direct predictors for the examples above, it is very convenient to simply include zip codes as predictors. However, zip codes  imply a very high-dimensional fixed effect and some may moreover contain very little exposure (i.e. very few observations). As a consequence, estimates may be very noisy for certain zip codes (or not defined) and fairly precise for others. Another problem is that zip codes inform us about where drivers live, but not necessarily about where they drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The traditional approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, actuaries use spatial smoothing to rein in the zip code effects. The (simplified) process works as follows:\n",
    "- Train a first GLM, such as $E[y|x] = \\exp(X' \\beta)$, to obtain a linear predictor $X'\\hat \\beta$.\n",
    "- Retrieve residuals for each zip code from this model: i.e. the zip code fixed effects from a second GLM with $X' \\hat \\beta$ as an offset.\n",
    "- Smooth the residuals with some kernel (e.g., Gaussian), taking into account both the exposure in each zip code and the distance between them.\n",
    "- Take the smoothed residuals and cluster them into a number of zones using, e.g., Ward clustering.\n",
    "- Take the zones, encode them into indicators $Z$ and fit a third GLM with them: $E[y|x] = \\exp(X' \\beta + Z' \\gamma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the smoothing work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "\n",
    "- $r_j$ be the residual of region $i$;\n",
    "- $d_{jj'}$ be the distance between regions $j$ and $j'$;\n",
    "- $e_j$ be exposure of region $j$;\n",
    "- $f(\\cdot)$ be a kernel for use in weighting.\n",
    " \n",
    "The smoothed residual is given by:\n",
    " \n",
    "\\begin{equation*}\n",
    "\\tilde {r_j} = w_j r_j + (1 - w_j) \\frac{\\sum_{j \\neq i} f(d_{jj'}) e_j r_j}{\\sum_{j \\neq i} f(d_{jj'}) e_j},\n",
    "\\end{equation*}\n",
    " \n",
    "where $w_j$ refers to the weight placed on one's own average:\n",
    " \n",
    "\\begin{equation*}\n",
    "w_j = \\left[\\frac{e_j}{\\sum_{j \\neq i} f(d_{jj'}) e_j}\\right] ^ \\rho.\n",
    "\\end{equation*}\n",
    " \n",
    "$f(\\cdot)$ can be any kernel; here, we use a Gaussian Kernel:\n",
    " \n",
    "\\begin{equation*}\n",
    "f(d) = \\exp\\left[ -\\frac{1}{2} \\left(\\frac{d}{h}\\right)^2 \\right],\n",
    "\\end{equation*}\n",
    "where $h$ is the bandwidth.\n",
    " \n",
    "In this form, the smoother has two hyperparameters: the bandwidth $h$ and a curvature parameter $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do we get the residuals from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the correct residual for smoothing?\n",
    " \n",
    "Index observations by $i$ and regions by $j$. All individuals in region $j$ are given by $I(j)$. The region of observation $i$ is given by $j(i)$.\n",
    " \n",
    "Using a log link function, the conditional expectation of $y_i/e_i$ is modeled as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb E[y_i / e_i | x_i] = \\exp(x_i' \\beta),\n",
    "\\end{equation*}\n",
    "\n",
    "where $e_i$ denotes exposure.\n",
    " \n",
    "We run this model, which gives us $\\hat{\\beta}$.\n",
    " \n",
    "So what is the correct residual to use? Note that we want the residual on the zip code level. So we _define_ the residual as the $\\{\\alpha_j\\}_j$ that solve\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb E[y_i / e_i | x_i] = \\exp(x_i' \\hat{\\beta} + \\alpha_j).\n",
    "\\end{equation*}\n",
    " \n",
    "How to estimate these $\\{\\alpha_j\\}_j$?\n",
    " \n",
    "For concreteness, consider a Tweedie GLM. Using $e_i$ to denote exposure, the interesting bit of the log-likelihood of a Tweedie GLM with power $p \\in (1, 2)$ is given by\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_i e_i \\left(\\frac{y_i}{e_i} \\frac{\\exp(x_i' \\hat{\\beta} + \\alpha_j) ^ {1-p}}{1-p} - \\frac{\\exp(x_i' \\hat{\\beta} + \\alpha_j) ^ {2 - p}}{2-p}\\right),\n",
    "\\end{equation*}\n",
    "\n",
    "where we use $e_i$ as weights. We skip the expression for $p=1$ and $p=2$, but the first order conditions are the same.\n",
    " \n",
    "The first order condition with respect to $\\alpha_j$ is given by, for all j:\n",
    " \n",
    "\\begin{equation*}\n",
    "\\sum_{i \\in I(j)} e_i \\left( \\frac{y_i}{e_i} \\exp(x_i' \\hat{\\beta} + \\alpha_j)^{1-p} - \\exp(x_i' \\hat{\\beta} + \\alpha_j)^{2-p} \\right) = 0.\n",
    "\\end{equation*}\n",
    " \n",
    "Re-arranging, we get\n",
    " \n",
    "\\begin{equation*}\n",
    "\\sum_{i \\in I(j)} y_i \\exp(x_i' \\hat{\\beta} + \\alpha_j)^{1-p} = \\sum_{i \\in I(j)} e_i \\exp(x_i' \\hat{\\beta} + \\alpha_j)^{2-p}\n",
    "\\end{equation*}\n",
    " \n",
    "and then\n",
    " \n",
    "\\begin{equation*}\\exp(\\alpha_j)^{1-p} \\sum_{i \\in I(j)} y_i \\exp(x_i' \\hat{\\beta})^{1-p} = \\exp(\\alpha_j)^{2-p} \\sum_{i \\in I(j)} e_i \\exp(x_i' \\hat{\\beta})^{2-p},\n",
    "\\end{equation*}\n",
    " \n",
    "which means that we can solve for $\\alpha_j$ directly:\n",
    " \n",
    "\\begin{equation*}\n",
    "\\alpha_j = \\log \\left( \\sum_{i \\in I(j)} y_i \\exp(x_i' \\hat{\\beta})^{1-p} \\right) - \\log \\left( \\sum_{i \\in I(j)} e_i \\exp(x_i' \\hat{\\beta})^{2-p} \\right).\n",
    "\\end{equation*}\n",
    " \n",
    "For Poisson ($p=1$), this simplfies to:\n",
    " \n",
    "\\begin{equation*}\n",
    "\\alpha_j = \\log \\left( \\sum_{i \\in I(j)} y_i \\right) - \\log \\left( \\sum_{i \\in I(j)} e_i \\exp(x_i' \\hat{\\beta}) \\right).\n",
    "\\end{equation*}\n",
    " \n",
    "For Gamma ($p=2$), this simplfies to:\n",
    " \n",
    "\\begin{equation*}\n",
    "\\alpha_j = \\log \\left( \\sum_{i \\in I(j)} y_i \\exp(-x_i' \\hat{\\beta}) \\right) - \\log \\left( \\sum_{i \\in I(j)} e_i \\right).\n",
    "\\end{equation*}\n",
    " \n",
    "Note that $\\alpha_j$ is a zip-code level fixed effect. However, unlike a typical fixed effect, it is estimated sequentially, i.e. we're not jointly estimating the main effects $\\beta$. To the extent that regional effects can be explained by $x_i$, the model will load on $\\beta$ not $\\alpha_j$.\n",
    " \n",
    "Note that for Poisson, we have a bit of an issue for all zip codes $j$ for which $y_i = 0$ for all $i \\in I(j)$, because the fixed effects would be $-\\infty$. As a first pass, we may just want to hard code a lower bound. It would be cleaner to use a Bayesian approach (or something similar to a Bayesian approach, such as a credibility method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Typical extensions of this approach involve using some geographic features (e.g., does the person live in a city? average income of the zip code?) in the first-stage model and only smoothing variation that is not easily explained by such variables. This procedure has the advantage of not treating areas that are very close in space but very different in their (observed) characteristics as equal or nearly so.\n",
    "- Other extensions involve different distance metrics:\n",
    "    - as the crow flies;\n",
    "    - actual driving times from centroid to centroid;\n",
    "    - neighborhood-based distance: how many borders do I have to cross to travel from $j$ to $j'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using Regularized GLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the multi-step procedure outlined above, we can also do all of this in one go. See Section 6 in [this tutorial](https://github.com/lorentzenchr/Tutorial_freMTPL2/blob/b99b688f4be3c50d9a3356cc95bc4504742040d0/glm_freMTPL2_example.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb E[y_{ij} / e_{ij} | x_{ij}] = \\exp(x_{ij}' \\beta + \\alpha_j)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use $i$ to index observations and $j$ to index regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we estimate the GLM, we use L2 regularization on the vector of zip code effects $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\min_{\\alpha, \\beta} -\\mathcal L + \\lambda \\alpha' P \\alpha\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathcal L$ is the log-likelihood, $\\alpha$ is a $J \\times 1$ vector, $P$ is $J \\times J$ matrix and $\\lambda$ is a scalar (which, for convenience, can also be incorporated into $P$). This is known as [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is straightforward to create $P$, but a little but of work. We want to regularize the differences between $\\alpha_j$ and $\\alpha_j'$. In doing so, the farther apart the regions $j$ and $j'$ and the greater the exposure in each of them, the more we let $\\alpha_j$ and $\\alpha_j'$ differ. The weight matrix $P$ conveys the (exposure-weighted) distance between zip codes, much as the kernel in the sequential procedure above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes this tricky?\n",
    "\n",
    "- We have very many zip codes ($\\approx 10,000$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps\n",
    "- Starting with [this tutorial](https://github.com/lorentzenchr/Tutorial_freMTPL2/blob/b99b688f4be3c50d9a3356cc95bc4504742040d0/glm_freMTPL2_example.ipynb), familiarize yourself with the problem and adapt it to work with our code base.\n",
    "- Extend the neighborhood-based distance metric used there to an actual distance based metric.\n",
    "- Compare the results with the traditional step-by-step approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Find (or generate) a data set for a prediction task that has a high dimensional regional fixed effect.\n",
    "- Figure out how to handle the large number of fixed effects (e.g., do not construct the full Hessian).\n",
    "- Construct the weight matrix $P$ (consider preserving sparsity for regions that are far apart).\n",
    "- Figure out how to tune effectively the various hyperparameters that we have ($\\alpha$ and the parameters that went into the construction of $P$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "- There's no need to restrict this to L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
