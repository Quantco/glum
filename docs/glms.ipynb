{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we talking about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following definitions follow [13].\n",
    "\n",
    "A generalized linear model (GLM) is a linear model ($\\eta = x^\\top \\beta$) wrapped in a transformation (link function) and equipped with a response distribution from an exponential family. The choice of link function and response distribution is very flexible. In a GLM, a predictive distribution for the response variable $Y$ is associated with a vector of observed predictors $x$.  The distribution has the form:\n",
    "\n",
    "\\begin{align}\n",
    "  p(y \\, |\\, x)\n",
    "&=\n",
    "  m(y, \\phi) \\exp\\left(\\frac{\\theta\\, T(y) - A(\\theta)}{\\phi}\\right)\n",
    "  & & \\textrm{random component / distribution family}\n",
    "\\\\\n",
    "  \\theta\n",
    "&:=\n",
    "  g(\\eta) && \\textrm{systematic component / link function}\n",
    "\\\\\n",
    "  \\eta\n",
    "&:=\n",
    "  x^\\top \\beta && \\textrm{parametric link component / linear predictor}\n",
    "\\end{align}\n",
    "\n",
    "Here $\\beta$ are the parameters; $\\phi$ is a parameter representing dispersion (\"variance\"); and $m$, $T$, $A$ are characterized by the user-specified model family; and $g$ is the **link function.** For background on the exponential family, and how common distributions can be represented in the form of the \"random component\" above, see [Wikipedia | Exponential family](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions).\n",
    "\n",
    "The expected mean of $Y$ depends on $x$ by composition of **linear response** $\\eta$ and (inverse) link function, i.e.:\n",
    "\n",
    "$$\n",
    "E[y | \\eta] := \\mu = g^{-1}(\\eta).\n",
    "$$\n",
    "\n",
    "Proving this involves use of [moment-generating functions and cumulants](https://en.wikipedia.org/wiki/Exponential_family#Moments_and_cumulants_of_the_sufficient_statistic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an offset, we write $\\eta = X^T \\beta + \\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of GLMs\n",
    "\n",
    "### Normal\n",
    "\n",
    "Many of the \"regression\" models we commonly work with are GLMs. For example, the normal distribution with a linear link function is a simple GLM: $y | x \\sim N(x^T \\beta, \\sigma^2)$. If we fit $\\beta$ with maximum likelihood, this is least-squares regression. If we replace the linear link function with an exponential link function, we get a simple multiplicative model that is also a GLM: $y | x \\sim N(e^{x^T \\beta}, \\sigma^2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweedie\n",
    "We usually work with a Tweedie distribution, which is a generalization of Poisson ($p=1$) and Gamma ($p=2$):\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta &= \\left\\{\\begin{array}{ll}\n",
    "\\frac{\\mu^{1-p}}{1-p} & \\text{if } p \\neq 1\\\\\n",
    "\\log \\mu              & \\text{if } p = 1\n",
    "\\end{array}\\right.,\\\\\n",
    "T(y) &= y, \\\\\n",
    "A(\\theta) &= \\left\\{\\begin{array}{ll}\n",
    "\\frac{\\mu^{2-p}}{2-p} & \\text{if } p \\neq 2\\\\\n",
    "\\log \\mu              & \\text{if } p = 2.\n",
    "\\end{array}\\right.,\n",
    "\\end{align*}\n",
    "\n",
    "With $1 < p < 2$, the Tweedie distribution is a [compound Poisson-gamma](https://en.wikipedia.org/wiki/Compound_Poisson_distribution#Compound_Poisson_Gamma_distribution) distribution. $y$ is distributed as if a number $N$ is drawn from a Poisson distribution, and then $N$ draws are taken IID from a gamma distribution and added. This distribution might model the total amount of a claim in an insurance context: There are $N$ incidents, and each incident $i$ has an amount $X_i$, and the total amount is the sum of $X_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit a GLM, we minimize the negative log-likelihood (or typically the unit deviance) subject to an elastic net constraint:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{\\beta} \\mathcal L + \\alpha \\rho ||\\beta||_1  + \\frac{\\alpha (1-\\rho)}{2} ||\\beta||_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also like to use the grouped lasso, which includes or excludes variables in groups. For example, if we have a categorical variable that we one-hot encode, like the make of a car, we might want to exclude all of the coefficients on that fixed effect together for greater interpretability and ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "Our objective function will generally be convex, because the log-likelihoods of members of the exponential family are convex in their \"natural parameterizations.\" The natural parameterization may not be the most obvious one. Example: The log-likelihood of the normal distribution is convex in one over the variance, which is its natural parameterization. It is not convex in the variance. We can generally assume we are solving convex problems.\n",
    "\n",
    "An exception is multicollinearity (rank deficiency) in the design matrix, without an L2 component to the penalty. In that case, the problem will be only weakly convex and will have no unique miminum. This is not an arcane consideration, since we frequently generate rank deficiency by constructing multiple sets of one-hot encoded categorical variables. This can make evaluating different optimizers tricky, since they could converge to different equally good optima. The glmnet paper suggests using at least a small l2 component to remove \"degeneracies and wild behavior.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRLS\n",
    "If we don't have a lasso penalty (i.e. $\\rho=1$), these problems will furthermore be differentiable. They are usually estimated using Iteratively Reweighted Least Squares (IRLS).\n",
    "\n",
    "IRLS can be justified in a couple of ways. First, and more intuitively, it can be seen as something something expected mean, something Taylor approximation. This setup works _only_ if the problem is set up as a GLM, and \n",
    "\n",
    "Second, the IRLS solution can be seen as taking a Newton step, but replacing the Hessian with the expected Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handy equations and derviatives:\\begin{align*}\n",
    "LL &= \\sum_i \\left( \\log(m(y_i, \\phi)) + \\frac{g(\\eta_i) T(y_i) - A(g(\\eta_i))}{\\phi} \\right) \\\\\n",
    "LL_i'(\\eta_i) &= \\frac{ g'(\\eta_i) \\left( T(y_i) - A'(g(\\eta_i)) \\right)}{\\phi} \\\\\n",
    "\\frac{\\partial LL}{\\partial \\beta_k} &= \\sum_i LL_i' \\frac{\\partial \\eta_i}{\\partial \\beta_k} \\\\\n",
    "&= \\sum_i LL_i' x_{ik}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "LL(\\tilde{\\beta}) &\\approx LL(\\beta) + \\sum_i \\left( LL_i'(\\eta_i) x_i (\\tilde{\\beta} - \\beta) + \n",
    "\\frac{1}{2} (x_i (\\tilde{\\beta} - \\beta))^2 LL_i''(\\eta_i)\n",
    "\\right) \\\\\n",
    " &= f(\\beta) + \\sum_i \\left(\n",
    " \\frac{1}{2} (x_i \\tilde{\\beta})^2 LL_i''\n",
    " + LL_i' x_i \\tilde{\\beta} \n",
    "- (x_i \\tilde{\\beta}) LL_i'' x_i \\beta\n",
    "\\right) \\\\\n",
    "&= f(\\beta) + \\sum_i \\left(\n",
    " \\frac{1}{2} (x_i \\tilde{\\beta})^2 LL_i''\n",
    " + x_i \\tilde{\\beta}  \\left( \n",
    "     LL_i' \n",
    "    - LL_i'' x_i \\beta\n",
    "    \\right)\n",
    "\\right) \\\\\n",
    "&= f(\\beta) + \\sum_i \\frac{1}{2} LL_i'' \\left(\n",
    " (x_i \\tilde{\\beta})^2 \n",
    " - 2 x_i \\tilde{\\beta}  \\left( \n",
    "   1-  \\frac{LL_i' }{LL_i''}\n",
    "    x_i \\beta \\right)\n",
    "\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letting $w_i = \\frac{1}{2} \\frac{\\partial^2 LL_i}{\\eta_i^2}$ and\n",
    "$z_i = 1 - \\frac{\\frac{\\partial LL_i}{\\partial \\eta_i}}{\\frac{\\partial LL_i^2}{\\partial \\eta_i^2}} x_i \\beta$,\n",
    "we can rewrite this as $LL(\\tilde{\\beta}) \\approx f(\\beta) + \\sum_i w_i \\left( x_i \\tilde{\\beta} - z_i \\right)^2$,\n",
    "and solve this like a least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the last term  equals $\\tilde{\\beta}^T x^T \\frac{\\partial LL}{\\partial \\beta} X \\beta$, and will be close to zero near the optimum since the gradient is zero near the optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to write terms with $\\beta$ in terms of $\\eta$,\n",
    "\\begin{align*}\n",
    "LL(\\tilde{\\beta}) &\\approx f(\\beta) + \\sum_i \\left(\n",
    "LL_i' x_i \\tilde{\\beta} \n",
    "+ \\frac{1}{2} LL_i'' \\left(\n",
    "(x_i \\tilde{\\beta})^2 - 2 x_i \\tilde{\\beta} (\\eta_i - \\gamma_i)\n",
    "\\right)\n",
    "\\right) \\\\\n",
    "&= f(\\beta) + \\sum_i \\frac{1}{2} LL_i'' \\left(\n",
    "2 \\frac{LL_i'}{LL_i''} \\left( x_i \\tilde{\\beta} \\right)\n",
    "+ \n",
    "(x_i \\tilde{\\beta})^2 - 2 x_i \\tilde{\\beta} (\\eta_i - \\gamma_i)\n",
    " \\right) \\\\\n",
    " &= f(\\beta) + \\sum_i \\frac{1}{2} LL_i'' \\left(\n",
    "(x_i \\tilde{\\beta})^2 - 2 x_i \\tilde{\\beta} \n",
    "\\left( \\eta_i - \\gamma_i \n",
    "- \\frac{LL_i'}{LL_i''}  \\right)\n",
    " \\right) \\\\\n",
    "  &= f(\\beta) + \\sum_i \\frac{1}{2} LL_i'' \\left(\n",
    "x_i \\tilde{\\beta} -  \n",
    "\\left( \\eta_i - \\gamma_i \n",
    "- \\frac{LL_i'}{LL_i''}  \\right)\n",
    " \\right)^2 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we don't have any current parameters? If current parameters are zero, this becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Descent\n",
    "With Lasso constraint, some version of coordinate descent is used for the optimization. Other optimization approaches exist (e.g. cvxpy), but we haven't seen these used in large-scale applications. The classic reference here is the \"glmnet paper\" [2].\n",
    "\n",
    "Coordinate Descent is older than the glmnet paper, and is a simple idea. In a problem with data $y$ and $x$ and parameters \"params\", Coordinate Descent involves repeatedly optimizing one or several parameters while holding the rest fixed.\n",
    "\n",
    "Interestingly, Wikipedia [claims](https://en.wikipedia.org/wiki/Coordinate_descent#Applications) that Coordinate Descent is often overlooked among researchers because it is simple to implement, and they would rather work on something more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cyclical coordinate descent with Newton-like steps and a\n",
    "twice-differentiable objective function.\n",
    "\"\"\"\n",
    "def coordinate_descent_inner(y, x, params, grad_func, hess_func):\n",
    "    for i, elt in enumerate(params):\n",
    "        step = -grad_func(y, x, params) / hess_func(y, x, params)\n",
    "        params[i] += step\n",
    "    return params\n",
    "        \n",
    "def coordinate_descent(y, x, params, grad_func, hess_func, \n",
    "                       convergence_func):\n",
    "    while not convergence_func():\n",
    "        params = coordinate_descent_inner(y, x, params, grad_func,\n",
    "                                         hess_func)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit more complicated when the objective function is not differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our use case in insurance, the critial features for GLMs to be useful for us are\n",
    "- Need to support Gamma, Poisson (pyglmnet, sklearn-fork, sklearn-master, h20, tfp, statsmodels)\n",
    "- Need to support sample weights (glmnet, sklearn-fork, sklearn-master, h2ostatsmodels)\n",
    "- Need to support offsets (an offset is a variable for which we force the parameter to equal 1). (glmnet, h2o, statsmodels)\n",
    "- Need to work on large-ish data sets (25 million rows, up to several hundred columns)\n",
    "- Can deal with high-dimensional categorical variables (either through sparse matrices or a dedicated solution) (sparse support: glmnet, h2o, tfp)\n",
    "- Need to be otherwise sensible (e.g. do not regularize the intercept, etc)\n",
    "- Decent performance in terms of CPU and memory (commercial competitor without regularization needs a couple of minutes on laptop)\n",
    "- Reliably convergent algorithms (_not_ tfp or sklearn-fork)\n",
    "- Support L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nice to have features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support Tweedie\n",
    "- Ability to specify custom weights for parameters for the L1 penalty\n",
    "- Ability to specify custom weight matrix for parameters for the L2 penalty (generalized Tikhonov regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which packages/implementations exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- glmnet (R-Package, Fortran implementation, GPL-2, no Tweedie, no gamma)\n",
    "https://glmnet.stanford.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pyglmnet (python only, no support of sample weights, no Tweedie, otherwise looks fairly feature-rich, under active development) https://github.com/glm-tools/pyglmnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- python-glmnet (python bindings to glmnet's Fortran code, not actively developed) https://github.com/civisanalytics/python-glmnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- glmnet_python (used at wayfair, GPL licensed) https://github.com/bbalasub1/glmnet_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scikit-learn fork (python only, has all the features we want, poor performance with sparse matrices, convergence trouble with lasso) https://github.com/scikit-learn/scikit-learn/pull/9405"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scikit-learn master (python only, only ridge, poor performnce with sparse matrices) https://github.com/scikit-learn/scikit-learn/pull/14300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- h2o (Java, python bindings, data needs to be copied from Python to Java all the time, convergence problems) http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensorflow Probability (C++, python bindings, no sample weights) https://www.tensorflow.org/probability/api_docs/python/tfp/glm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- statsmodels (python only, very poor performance, lots of issues getting it to work) https://www.statsmodels.org/stable/glm.html\n",
    "- [celer](https://github.com/mathurinm/celer): Fast solver for Lasso and sparse logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other potentially useful reading material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1] Book: [Generalized Linear Models](http://www.utstat.toronto.edu/~brunner/oldclass/2201s11/readings/glmbook.pdf), McCullagh and Nelder\n",
    "- [2] [Regularization Paths for GEneralized Linear Models via Coordinate Descent](http://statweb.stanford.edu/~jhf/ftp/glmnet.pdf). This is the \"glmnet paper\" that describes how the glment package works and the popular Coordinate Descent algorithm.\n",
    "- [3] [Tensorflow GLM Tutorial](https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Generalized_Linear_Models.ipynb)\n",
    "- [4] [Sklearn User guide](https://scikit-learn.org/dev/modules/linear_model.html#generalized-linear-regression)\n",
    "- [5] [Tutorial on Insurance Claims Modelling with sklearn](https://github.com/lorentzenchr/Tutorial_freMTPL2/blob/master/glm_freMTPL2_example.ipynb)\n",
    "- [6] [pyglmnet Tutorial](http://glm-tools.github.io/pyglmnet/tutorial.html)\n",
    "- [7] [Improved GLMNET](https://www.csie.ntu.edu.tw/~cjlin/papers/l1_glmnet/long-glmnet.pdf)\n",
    "- [8] [h2o Documentation on GLMs (fairly extensive)](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html)\n",
    "- [9] [A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification](https://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf), Yuan, Chang, Hsieh, and Lin 2010.\n",
    "- [10] [An Improved GLMNET for L1-regularized Logistic Regression](http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf), Yuan, Ho, and Lin 2012. Is this the basis for the sklearn package's \"improved\" glmnet? Or tfp.glm.fit_sparse?\n",
    "- [11] Worked example of Tweedie regression on insurance claims from [sklearn](https://github.com/scikit-learn/scikit-learn/blob/98054bc9a4416c49b26a3a253b9a7bef16a1e27b/examples/linear_model/plot_tweedie_regression_insurance_claims.py)\n",
    "- [12] Write-up on [exposure](https://quantcoteam.slack.com/archives/C010MC8RHBK/p1585680163082400)\n",
    "- [13] [Slides](https://www.groups.ma.tum.de/fileadmin/w00ccg/statistics/czado/lec2.pdf) on GLMs and IRLS from Claudia Czado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
